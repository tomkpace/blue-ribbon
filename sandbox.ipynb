{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bitbaseconda695c26050d2f45bb8896ce2b0036a0bd",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "63d0a0a306d17f9618fcd2df71f8943413e3c03229e5631c7d32ac919a8133cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from relation_extractor import RelationExtractor, negation_detector, value_smoother\n",
    "from nlp import nlp, ner, process_ner_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = pd.read_csv(\"raw_data/movie_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I noticed that there were duplicate entries for certain \n",
    "movies that were released in separate origins.  This \n",
    "removes those duplicates so we can use the rotten_tomatoes_link\n",
    "as the primary index of the data.\n",
    "\"\"\"\n",
    "duplicates = movie_data.groupby(\"rotten_tomatoes_link\").count() \\\n",
    "    .loc[np.any(movie_data.groupby(\"rotten_tomatoes_link\") \\\n",
    "    .count() > 1, axis=1)].index\n",
    "\n",
    "dup_indices = movie_data.loc[(movie_data[\"rotten_tomatoes_link\"].isin(duplicates)) & \\\n",
    "    (movie_data[\"Origin/Ethnicity\"] != \"American\")].index\n",
    "\n",
    "movie_data = movie_data.loc[~movie_data.index.isin(dup_indices)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_column(input_df, column_name, new_column_name):\n",
    "    \"\"\"\n",
    "    Function that will expand the string columns that have \n",
    "    values separated by commas.\n",
    "    \"\"\"\n",
    "    df = input_df.copy(deep=True)\n",
    "    exploded_df = df[column_name].astype(str) \\\n",
    "        .apply(lambda x: x.split(\",\")).explode() \\\n",
    "        .to_frame().rename(columns={column_name: new_column_name})\n",
    "    df = df.merge(exploded_df[[new_column_name]], left_index=True, right_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "explode_columns = {\n",
    "    \"genres\": \"genre\",\n",
    "    \"directors\": \"director\",\n",
    "    \"authors\": \"author\",\n",
    "    \"actors\": \"actor\"\n",
    "}\n",
    "\n",
    "exploded_dfs = {}\n",
    "for c in explode_columns.keys():\n",
    "    exploded_dfs[c] = explode_column(movie_data, c, explode_columns[c])\n",
    "for c in exploded_dfs.keys():\n",
    "    movie_data = movie_data.merge(\n",
    "        exploded_dfs[c][[explode_columns[c]]], left_index=True, right_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_dict = {\n",
    "    \"rotten_tomatoes_link\":\"entity_id\",\n",
    "    \"Title\":\"has title\",\n",
    "    \"director\":\"directed by\",\n",
    "    \"author\":\"authored by\",\n",
    "    \"actor\":\"featured actor\",\n",
    "    \"genre\":\"has genre\",\n",
    "    \"Release Year\":\"released on\",\n",
    "    \"production_company\":\"produced by\",\n",
    "}\n",
    "relations = []\n",
    "for r in relation_dict:\n",
    "    relations.append(relation_dict[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_df = movie_data.rename(columns=relation_dict)[relations] \\\n",
    "    .drop_duplicates() \\\n",
    "    .reset_index(drop=True)\n",
    "known_df = tabular_df.melt(id_vars=\"entity_id\", value_vars=['has title', 'directed by', 'authored by', 'featured actor',\n",
    "       'has genre', 'released on', 'produced by']).drop_duplicates().reset_index(drop=True).rename(columns={\"variable\":\"relation\"})\n",
    "known_df[\"value\"] = known_df[\"value\"].astype(str).apply(value_smoother)\n",
    "known_df.to_csv(\"data/known_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_review_df = pd.read_csv(\"raw_data/rotten_tomatoes_critic_reviews.csv\")\n",
    "raw_review_df = raw_review_df.loc[raw_review_df[\"review_content\"].astype(str) != 'nan'].reset_index(drop=True)\n",
    "raw_review_df.to_csv(\"data/raw_review_df.csv\")\n",
    "review_df = raw_review_df[[\"rotten_tomatoes_link\", \"review_content\"]] \\\n",
    "    .rename(columns={\"rotten_tomatoes_link\": \"entity_id\", \"review_content\": \"text\"})\n",
    "review_df.to_csv(\"data/review_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_plot_df = pd.read_csv(\"raw_data/data/wiki_movie_plots_deduped.csv\")\n",
    "plot_df = raw_plot_df.merge(movie_data.rename(columns={\n",
    "    \"rotten_tomatoes_link\": \"entity_id\"})[[\"entity_id\", \"Title\"]] \\\n",
    "    .drop_duplicates(), on=\"Title\") \\\n",
    "    .rename(columns={\"Plot\": \"text\"})[[\"entity_id\", \"text\"]]\n",
    "plot_df.to_csv(\"data/plot_df.csv\", index=False)\n",
    "review_plot_df = pd.concat([review_df, plot_df]).reset_index(drop=True)\n",
    "review_plot_df.to_csv(\"data/review_plot_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv(\"data/review_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34372\nOur familiarity with the actors, and their comfort in this period setting, lend the piece an unexpected air of naturalism.\n"
     ]
    }
   ],
   "source": [
    "i = np.random.randint(1, len(review_df))\n",
    "text = review_df.iloc[i][\"text\"]\n",
    "entity_id = review_df.iloc[i][\"entity_id\"]\n",
    "print(i)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fairbrass True\n* fairbrass\nboth the drama True\n* both\n* the\n* drama\nscenes True\n* scenes\nhe True\n* he\nparts True\n* parts\nJason Statham movies True\n* jason\n* statham\n* movies\nhis calling True\n* his\n* calling\n____________________________________________________\nFairbrass nsubj NOUN False\nmoves ROOT VERB False\nstiffly acomp VERB False\nthrough prep ADP True\nboth preconj CCONJ True\nthe det DET True\ndrama pobj NOUN False\nand cc CCONJ True\nfight conj VERB False\nscenes dobj NOUN False\n- punct PUNCT False\nas mark ADP True\nif mark ADP True\nhe nsubj PRON True\nwere advcl VERB True\nunderwater acomp ADJ False\n. punct PUNCT False\nSupporting csubj VERB False\nparts dobj NOUN False\nin prep ADP True\nJason compound PROPN False\nStatham compound PROPN False\nmovies pobj NOUN False\nmay aux VERB True\nbe ROOT VERB True\nhis poss DET True\ncalling attr NOUN False\n, punct PUNCT False\ninstead advmod ADV False\n. punct PUNCT False\n"
     ]
    }
   ],
   "source": [
    "for n in nlp(text).noun_chunks:\n",
    "    print(n, not negation_detector(n.root))\n",
    "    for t in n:\n",
    "        print(\"*\", t.text.lower())\n",
    "print(\"____________________________________________________\")\n",
    "for t in nlp(text):\n",
    "    print(t.text, t.dep_, t.pos_, t.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Our\nfamiliarity\nthe\nactors\ntheir\ncomfort\nthis\nperiod\nthe\npiece\nan\nunexpected\nair\nnaturalism\n('entity_id', 'features the theme', 'our familiarity')\n('entity_id', 'features the theme', 'the actors')\n('entity_id', 'features the theme', 'their comfort')\n('entity_id', 'features the theme', 'this period')\n('entity_id', 'features the theme', 'the piece')\n('entity_id', 'features the theme', 'an unexpected air')\n('entity_id', 'features the theme', 'naturalism')\n"
     ]
    }
   ],
   "source": [
    "r = RelationExtractor(\"entity_id\", text)\n",
    "for i in r.relations:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('entity_id', 'features the theme', 'fairbrass'),\n",
       " ('entity_id', 'features the theme', 'both the drama'),\n",
       " ('entity_id', 'features the theme', 'scenes'),\n",
       " ('entity_id', 'features the theme', 'he'),\n",
       " ('entity_id', 'features the theme', 'parts'),\n",
       " ('entity_id', 'features the theme', 'jason statham movies'),\n",
       " ('entity_id', 'features the theme', 'his calling')]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "def extract_noun_relations(entity_id, text):\n",
    "    doc = nlp(text)\n",
    "    relations = []\n",
    "    for n in doc.noun_chunks:\n",
    "        noun_chunk = []\n",
    "        if not negation_detector(n.root):\n",
    "            for t in n:\n",
    "                noun_chunk.append(t.text.lower())\n",
    "            relations.append(\n",
    "                (entity_id, \"features the theme\", \" \".join(noun_chunk))\n",
    "            )\n",
    "    return relations\n",
    "\n",
    "extract_noun_relations(\"entity_id\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'A lot of bad stuff'"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "text = ' \"A lot of bad stuff'\n",
    "def value_smoother(text):\n",
    "    text = text.replace(\" -\", \"-\")\n",
    "    text = text.replace(\"- \", \"-\")\n",
    "    text = text.replace(\"' s\", \"'s\")\n",
    "    if text.count('\"') == 1:\n",
    "        text = text.replace('\"', \"\")\n",
    "    while text[0] == \" \":\n",
    "        text = text[1:]\n",
    "    return text\n",
    "value_smoother(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[A, lot, of, bad, stuff]\n[bad, stuff]\n"
     ]
    }
   ],
   "source": [
    "def find_composite_noun_chunk(noun_chunk):\n",
    "    composite_noun_chunk_idx = set()\n",
    "    composite_noun_chunk = []\n",
    "    for n in noun_chunk.doc.noun_chunks:\n",
    "        if n == noun_chunk:\n",
    "            for t in n:\n",
    "                composite_noun_chunk_idx.add(t.i)\n",
    "        elif n.root.head.head == noun_chunk.root:\n",
    "            for t in n:\n",
    "                composite_noun_chunk_idx.add(t.i)\n",
    "            composite_noun_chunk_idx.add(n.root.head.i)\n",
    "    for t in noun_chunk.doc:\n",
    "        if t.i in composite_noun_chunk_idx:\n",
    "            composite_noun_chunk.append(t)\n",
    "    return composite_noun_chunk\n",
    "\n",
    "\n",
    "for n in doc.noun_chunks:\n",
    "    print(find_composite_noun_chunk(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_plot_df = pd.read_csv(\"raw_data/data/wiki_movie_plots_deduped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_plot_df = pd.read_csv(\"raw_data/data/wiki_movie_plots_deduped.csv\")\n",
    "plot_df = raw_plot_df.merge(movie_data.rename(columns={\n",
    "    \"rotten_tomatoes_link\": \"entity_id\"})[[\"entity_id\", \"Title\"]] \\\n",
    "    .drop_duplicates(), on=\"Title\") \\\n",
    "    .rename(columns={\"Plot\": \"text\"})[[\"entity_id\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doesn't really work.\n",
    "class NounChunks:\n",
    "    def __init__(self, doc):\n",
    "        self.doc = doc\n",
    "        self.details_list = self.generate_details_list()\n",
    "        self.roots = self.generate_roots()\n",
    "        self.values = self.generate_values()\n",
    "        self.spans = self.generate_spans()\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_noun_chunk_details(noun_chunk):\n",
    "        details = {}\n",
    "        details[\"root\"] = noun_chunk.root\n",
    "        details[\"span\"] = list(range(noun_chunk.start, noun_chunk.end))\n",
    "        details[\"text\"] = noun_chunk.text\n",
    "        return details\n",
    "        \n",
    "    def generate_details_list(self):\n",
    "        details_list = []\n",
    "        for n in doc.noun_chunks:\n",
    "            details_list.append(\n",
    "                self.extract_noun_chunk_details(n)\n",
    "            )\n",
    "        return details_list\n",
    "    def generate_roots(self):\n",
    "        roots = []\n",
    "        for d in self.details_list:\n",
    "            roots.append(d[\"root\"])\n",
    "        return roots\n",
    "    def generate_spans(self):\n",
    "        spans = []\n",
    "        for d in self.details_list:\n",
    "            spans += d[\"span\"]\n",
    "        return spans\n",
    "    def generate_values(self):\n",
    "        values = []\n",
    "        for d in self.details_list:\n",
    "            values.append(d[\"text\"])\n",
    "        return values\n",
    "    def generate_relations(self, entity_id):\n",
    "        relations = []\n",
    "        for n in self.doc.noun_chunks:\n",
    "            if n.root.head.i in self.spans:\n",
    "                base_relation = n.root.head.head\n",
    "                relation = [n.root.head.head.text]\n",
    "            else:\n",
    "                base_relation = n.root.head\n",
    "                relation = [n.root.head.text]\n",
    "            if base_relation.dep_ == \"prep\" and base_relation.head.pos_ == \"VERB\":\n",
    "                relation.append(base_relation.head.text)\n",
    "            relation.reverse()\n",
    "            relation = \" \".join(relation)\n",
    "            relations.append(relation.lower())\n",
    "        relations_dict = {}\n",
    "        relations_dict[\"entity_id\"] = entity_id\n",
    "        relations_dict[\"relation\"] = relations\n",
    "        relations_dict[\"value\"] = self.values\n",
    "        relations_df = pd.DataFrame().from_dict(relations_dict)\n",
    "        return relations_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
       "                     criterion='gini', max_depth=None, max_features='auto',\n",
       "                     max_leaf_nodes=None, max_samples=None,\n",
       "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                     min_samples_leaf=1, min_samples_split=2,\n",
       "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
       "                     warm_start=False)"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "def generate_theme_df(movie_data):\n",
    "    theme_df = movie_data[[\"rotten_tomatoes_link\", \"Title\", \"genre\", \"Plot\"]] \\\n",
    "        .drop_duplicates().reset_index(drop=True)\n",
    "    genres = theme_df[\"genre\"].unique()\n",
    "    theme_df[\"indicator\"] = 1\n",
    "    theme_pivot = theme_df \\\n",
    "        .pivot(index=\"rotten_tomatoes_link\", columns=\"genre\", values=\"indicator\") \\\n",
    "        .fillna(0).reset_index()\n",
    "    theme_df = theme_df[[\"rotten_tomatoes_link\", \"Title\", \"Plot\"]] \\\n",
    "        .drop_duplicates().merge(theme_pivot, on=\"rotten_tomatoes_link\") \\\n",
    "        .reset_index(drop=True)\n",
    "    return theme_df\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,3))\n",
    "clf = ExtraTreesClassifier()\n",
    "X = vectorizer.fit_transform(theme_df[\"Plot\"])\n",
    "clf.fit(X, theme_df[\"Comedy\"])\n",
    "importances = clf.feature_importances_\n",
    "names = vectorizer.get_feature_names()"
   ]
  }
 ]
}